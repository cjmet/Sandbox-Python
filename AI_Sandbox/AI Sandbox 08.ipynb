{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP - Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cad5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['I', 'love', 'coding', '!', 'Python', 'is', 'fun', '.', 'Let', \"'s\", 'learn', 'NLP', '.']\n",
      "Sentence Tokens: ['I love coding!', 'Python is fun.', \"Let's learn NLP.\"]\n",
      "POS Tags: [('I', 'PRP'), ('love', 'VBP'), ('coding', 'VBG'), ('in', 'IN'), ('Python', 'NNP')]\n",
      "\n",
      "Common POS Tags:\n",
      "NN: Noun, VB: Verb, JJ: Adjective, PRP: Pronoun\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\khaai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\khaai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download(\"punkt\") <- Deprecated\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "def print_divider():\n",
    "    print(\"-\" * 50, \"\\n\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I love coding! Python is fun. Let's learn NLP.\"\n",
    "\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n",
    "\n",
    "\n",
    "# Download POS tagger model\n",
    "#nltk.download('averaged_perceptron_tagger_eng') <- Deprecated\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love coding in Python\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Common POS tags reference\n",
    "print(\"\\nCommon POS Tags:\")\n",
    "print(\"NN: Noun, VB: Verb, JJ: Adjective, PRP: Pronoun\")\n",
    "print_divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a83343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stop Words Removal: ['quick', 'brown', 'foxes', 'running', 'faster']\n",
      "After Stemming: ['quick', 'brown', 'fox', 'run', 'faster']\n",
      "After Lemmatization: ['quick', 'brown', 'fox', 'running', 'faster']\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download stop words and lemmatizer data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown foxes are running faster\"\n",
    "tokens = nltk.word_tokenize(text.lower()) # Convert to lowercase\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"After Stop Words Removal:\", filtered_tokens)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"After Stemming:\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"After Lemmatization:\", lemmatized_tokens)\n",
    "print_divider()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary: ['coding' 'for' 'fun' 'great' 'in' 'is' 'love' 'nlp' 'python']\n",
      "BoW Matrix:\n",
      " [[1 0 0 0 1 0 1 0 1]\n",
      " [0 1 0 1 0 1 0 1 1]\n",
      " [1 0 1 0 0 1 0 0 0]]\n",
      "\n",
      "TF-IDF Vocabulary: ['coding' 'for' 'fun' 'great' 'in' 'is' 'love' 'nlp' 'python']\n",
      "TF-IDF Matrix:\n",
      " [[0.42804604 0.         0.         0.         0.5628291  0.\n",
      "  0.5628291  0.         0.42804604]\n",
      " [0.         0.49047908 0.         0.49047908 0.         0.37302199\n",
      "  0.         0.49047908 0.37302199]\n",
      " [0.51785612 0.         0.68091856 0.         0.         0.51785612\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "TextRank Summary: Python is widely used in NLP. NLTK is a popular library for NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "# Install scikit-learn if needed\n",
    "# !pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    " \"I love coding in Python\",\n",
    " \"Python is great for NLP\",\n",
    " \"Coding is fun\"\n",
    "]\n",
    "\n",
    "# Bag of Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(docs)\n",
    "print(\"BoW Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "print(\"\\nTF-IDF Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Simple TextRank Summarizer\n",
    "def textrank_summary(text, num_sentences=2):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    " \n",
    "    # Create TF-IDF matrix for sentences\n",
    "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = (tfidf * tfidf.T).toarray()\n",
    "    # Rank sentences using PageRank\n",
    "    scores = np.array([sum(row) for row in similarity_matrix])\n",
    "    top_indices = scores.argsort()[-num_sentences:][::-1]\n",
    "    summary = ' '.join([sentences[i] for i in sorted(top_indices)])\n",
    "    return summary\n",
    "\n",
    "# Sample text for summarization\n",
    "text = \"Natural language processing is a field of AI. It enables computers to understand human language. Python is widely used in NLP. NLTK is a popular library for NLP tasks.\"\n",
    "summary = textrank_summary(text)\n",
    "print(\"\\nTextRank Summary:\", summary)\n",
    "print_divider()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d109e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['fun', 'nlp', 'great', 'coding', 'python']\n",
      "Topic 2: ['sport', 'like', 'sports', 'exciting', 'football']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    " \"Python is great for coding and NLP\",\n",
    " \"Football is a popular sport\",\n",
    " \"Coding in Python is fun\",\n",
    " \"Sports like football are exciting\"\n",
    "]\n",
    "\n",
    "# Preprocess and vectorize\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(doc_matrix)\n",
    "\n",
    "# Display topics\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lda.components_):\n",
    " top_words = [words[j] for j in topic.argsort()[-5:]]\n",
    " print(f\"Topic {i+1}: {top_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9967e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Options\n",
    "rate_limit_seconds = 3  # API Rate limit in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae21ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from mistralai import Mistral  # Install via pip if needed: pip install mistralai\n",
    "\n",
    "def divider(): \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Set your API key (replace with your provided key)\n",
    "# os.environ[\"MISTRAL_API_KEY\"] = \"your_api_key_here\"\n",
    "# I set it in the environment for security.\n",
    "\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    raise ValueError(\"MISTRAL_API environment variable is not set. Please set it to your API key.\")\n",
    "else : \n",
    "    print(\"API Key is set.\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Function to generate a response using Mistral API\n",
    "\n",
    "# set rate_limit_timeout to now\n",
    "rate_limit_timeout = 0  # When the rate limit will expire\n",
    "\n",
    "def generate_response(prompt, model=\"mistral-small-latest\", temperature=0.7, max_tokens=200, top_p=1.0, seed=None, debug=False):\n",
    "    \"\"\"\n",
    "    Updated modular function to generate a response from Mistral.ai.\n",
    "    - prompt: The input prompt.\n",
    "    - model: LLM variant (e.g., 'mistral-small-latest' for basics).\n",
    "    - temperature: Randomness level (0.0: deterministic, 1.0: creative).\n",
    "    - max_tokens: Limits output length.\n",
    "    - top_p: Nucleus sampling threshold (0-1; lower values restrict to more probable tokens).\n",
    "    Returns the generated text.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Generating response with model: {model}, temperature: {temperature}, max_tokens: {max_tokens}, top_p: {top_p}, seed: {seed}\")\n",
    "\n",
    "    # Rate limit\n",
    "    global rate_limit_seconds\n",
    "    global rate_limit_timeout\n",
    "    now = time.time()\n",
    "    dif = rate_limit_timeout - now   \n",
    "\n",
    "    # Spinner \n",
    "    if (dif > 0):\n",
    "        # Spinner Countdown   \n",
    "        while dif > 0:\n",
    "            print(f\"\\rRate limit {dif:5.3f}s\", end='', flush=True)\n",
    "            time.sleep(0.25)  # Update spinner every 0.25 seconds\n",
    "            dif = rate_limit_timeout - time.time()\n",
    "        print(\"\\r                         \\r\", end='', flush=True)\n",
    "        # /Spinner \n",
    "\n",
    "    # Set the Rate Limit    \n",
    "    rate_limit_timeout = time.time() + rate_limit_seconds\n",
    "    \n",
    "    client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "    try:\n",
    "        response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,  # Added for nucleus sampling support\n",
    "        random_seed=seed  # Optional: for reproducibility\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# /------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03868577",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):  # Set to True to run examples\n",
    "    # Example: Basic prompt\n",
    "    basic_prompt = \"Explain AI in one sentence.\"\n",
    "    response = generate_response(basic_prompt)\n",
    "    print(\"Basic Response:\", response)\n",
    "    divider()\n",
    "\n",
    "    # Example: Zero-shot, few-shot, and CoT prompts\n",
    "    zero_shot_prompt = \"Classify this sentiment: 'I love this product!'\"\n",
    "    zero_response = generate_response(zero_shot_prompt, temperature=0.5)\n",
    "    print(\"Zero-shot:\", zero_response)\n",
    "    divider()\n",
    "\n",
    "    # Few-shot example\n",
    "    few_shot_prompt = \"\"\"\n",
    "    Example 1: Text: 'Great movie.' Sentiment: Positive.\n",
    "    Example 2: Text: 'Terrible service.' Sentiment: Negative.\n",
    "    Classify: 'Okay experience.'\n",
    "    \"\"\"\n",
    "    few_response = generate_response(few_shot_prompt, top_p=0.9)  # Now supported\n",
    "    print(\"Few-shot:\", few_response)\n",
    "    divider()\n",
    "\n",
    "    # Chain of Thought (CoT) example\n",
    "    cot_prompt = \"Solve: What is 15% of 200? Think step by step.\"\n",
    "    cot_response = generate_response(cot_prompt, temperature=0.2)\n",
    "    print(\"CoT:\", cot_response)\n",
    "    divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6807eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response with model: mistral-small-latest, temperature: 0.7, max_tokens: 200, top_p: 1.0, seed: 42\n",
      "Seeded Response: [ Here’s a randomly generated number between 1 and 1000:\n",
      "\n",
      "**742** ]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example random seed test \n",
    "seeded_prompt = \"Generate a random number between 1 and 1000.\"\n",
    "seeded_response = generate_response(seeded_prompt, seed=42, debug=True)\n",
    "print(\"Seeded Response: [\", seeded_response, \"]\")\n",
    "divider()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe1a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response with model: mistral-small-latest, temperature: 0.7, max_tokens: 200, top_p: 1.0, seed: 42\n",
      "Seeded Response: [ Here’s a randomly generated number between 1 and 1000:\n",
      "\n",
      "**742** ]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example random seed test \n",
    "seeded_prompt = \"Generate another random number between 1 and 1000.\"\n",
    "seeded_response = generate_response(seeded_prompt, seed=42, debug=True)\n",
    "print(\"Seeded Response: [\", seeded_response, \"]\")\n",
    "divider()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7819d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of recent edits\n"
     ]
    }
   ],
   "source": [
    "print(\"End of recent edits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

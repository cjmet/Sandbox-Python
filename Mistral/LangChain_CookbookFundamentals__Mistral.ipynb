{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "359697d5",
      "metadata": {
        "id": "359697d5"
      },
      "source": [
        "# LangChain Cookbook ðŸ‘¨â€ðŸ³ðŸ‘©â€ðŸ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9815081",
      "metadata": {
        "executionInfo": {
          "elapsed": 16,
          "status": "ok",
          "timestamp": 1756165466856,
          "user": {
            "displayName": "Aswath Rachuru",
            "userId": "15312901110463830063"
          },
          "user_tz": 240
        },
        "hide_input": false,
        "id": "e9815081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that comes after Friday is Saturday.\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "\n",
        "# # mistral_api_key=os.getenv('MISTRAL_API_KEY', 'use your key')\n",
        "\n",
        "# MISTRAL_API_KEY = '...'\n",
        "\n",
        "# Cell 1 â€” config\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\", \"use your key\")\n",
        "os.environ[\"MISTRAL_API_KEY\"] = mistral_api_key  # make it visible to the SDK\n",
        "# Cell 2 â€” model (no api_key arg needed)\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
        "# Cell 3 â€” call\n",
        "resp = chat.invoke([\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"What day comes after Friday?\")\n",
        "])\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0dc06c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 28,
          "status": "ok",
          "timestamp": 1756165475296,
          "user": {
            "displayName": "Aswath Rachuru",
            "userId": "15312901110463830063"
          },
          "user_tz": 240
        },
        "hide_input": false,
        "id": "8e0dc06c",
        "outputId": "33597de5-87ba-4ad0-f8c1-0697f3c57c24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What day comes after Friday?'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # You'll be working with simple strings (that'll soon grow in complexity!)\n",
        "# my_text = \"What day comes after Friday?\"\n",
        "# my_text\n",
        "\n",
        "# # !pip install langchain_mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b0935b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "executionInfo": {
          "elapsed": 1298,
          "status": "error",
          "timestamp": 1756165482673,
          "user": {
            "displayName": "Aswath Rachuru",
            "userId": "15312901110463830063"
          },
          "user_tz": 240
        },
        "id": "99b0935b",
        "outputId": "28f87f5f-4118-4554-9dd2-fe09ea91ffe2"
      },
      "outputs": [],
      "source": [
        "# from langchain_mistralai import ChatMistralAI\n",
        "# from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# # Create a Mistral chat model (uses MISTRAL_API_KEY from your environment)\n",
        "# chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d2f7af",
      "metadata": {
        "id": "a2d2f7af"
      },
      "source": [
        "Now let's create a few messages that simulate a chat experience with a bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878d6a36",
      "metadata": {
        "id": "878d6a36",
        "outputId": "9cb551a0-1c43-42c2-9060-9239bc12a8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that comes after Friday is Saturday.\n"
          ]
        }
      ],
      "source": [
        "# from langchain_mistralai import ChatMistralAI\n",
        "# from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# # Pass the key directly here ðŸ‘‡\n",
        "# chat = ChatMistralAI(\n",
        "#     model=\"mistral-small-latest\",\n",
        "#     temperature=0.7,\n",
        "#     api_key=\"use your key\"\n",
        "# )\n",
        "\n",
        "# resp = chat.invoke([\n",
        "#     SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "#     HumanMessage(content=\"What day comes after Friday?\")\n",
        "# ])\n",
        "\n",
        "# print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a425aaa",
      "metadata": {
        "id": "0a425aaa"
      },
      "source": [
        "You can also pass more chat history w/ responses from the AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8fd3fe88",
      "metadata": {
        "id": "8fd3fe88",
        "outputId": "93f10b07-3e46-4a97-b3c4-46ec7f89ad68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explore the historic old town, visit the MusÃ©e Matisse, and take a day trip to Monaco.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "resp = chat.invoke([\n",
        "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
        "    HumanMessage(content=\"I like the beaches where should I go?\"),\n",
        "    AIMessage(content=\"You should go to Nice, France\"),\n",
        "    HumanMessage(content=\"What else should I do when I'm there?\")\n",
        "])\n",
        "\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5ee37a",
      "metadata": {
        "id": "ff5ee37a"
      },
      "source": [
        "You can also exclude the system message if you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "238a49f6",
      "metadata": {
        "id": "238a49f6",
        "outputId": "f0b66749-ca98-4e0d-f39d-0e25f240f768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that comes after Thursday is **Friday**.\n",
            "\n",
            "Hereâ€™s the sequence of days for reference:\n",
            "- Monday\n",
            "- Tuesday\n",
            "- Wednesday\n",
            "- Thursday\n",
            "- **Friday**\n",
            "- Saturday\n",
            "- Sunday\n"
          ]
        }
      ],
      "source": [
        "resp = chat.invoke(\"What day comes after Thursday?\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bf9634",
      "metadata": {
        "id": "66bf9634"
      },
      "source": [
        "### **Documents**\n",
        "An object that holds a piece of text and metadata (more information about that text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3bbf58b2",
      "metadata": {
        "id": "3bbf58b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema import Document\n",
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "         metadata={\n",
        "             'my_document_id' : 234234,\n",
        "             'my_document_source' : \"The LangChain Papers\",\n",
        "             'my_document_create_time' : 1680013019\n",
        "         })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd19754",
      "metadata": {
        "id": "3bd19754"
      },
      "source": [
        "But you don't have to include metadata if you don't want to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0798d3ca",
      "metadata": {
        "id": "0798d3ca",
        "outputId": "774ca0e6-79ba-4e90-a1a6-e29ba75816b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e462b5d",
      "metadata": {
        "id": "2e462b5d"
      },
      "source": [
        "## Models - The interface to the AI brains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b27fe982",
      "metadata": {
        "id": "b27fe982"
      },
      "source": [
        "###  **Language Model**\n",
        "A model that does text in âž¡ï¸ text out!\n",
        "\n",
        "*Check out how I changed the model I was using from the default one to ada-001 (a very cheap, low performing model). See more models [here](https://platform.openai.com/docs/models)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "74b1a72a",
      "metadata": {
        "id": "74b1a72a"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Correct: use a valid Mistral model; remove OpenAI's model_name and the stray \"mistral_\"\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "46e193da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that comes after Friday is **Saturday**.\n",
            "\n",
            "Hereâ€™s the sequence for reference:\n",
            "- Sunday\n",
            "- Monday\n",
            "- Tuesday\n",
            "- Wednesday\n",
            "- Thursday\n",
            "- **Friday**\n",
            "- **Saturday**\n",
            "- Sunday\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "resp = llm.invoke([HumanMessage(content=\"What day comes after Friday?\")])\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6399c295",
      "metadata": {
        "id": "6399c295",
        "outputId": "ac570224-abd9-43dd-f688-fe54c1359f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that comes after Friday is **Saturday**.\n",
            "\n",
            "Hereâ€™s the sequence of days for reference:\n",
            "- Sunday\n",
            "- Monday\n",
            "- Tuesday\n",
            "- Wednesday\n",
            "- Thursday\n",
            "- **Friday**\n",
            "- **Saturday**\n",
            "- Sunday\n",
            "\n",
            "So, after Friday, the next day is Saturday.\n"
          ]
        }
      ],
      "source": [
        "resp = llm.invoke(\"What day comes after Friday?\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef89bfa",
      "metadata": {
        "id": "3ef89bfa"
      },
      "source": [
        "### **Chat Model**\n",
        "A model that takes a series of messages and returns a message output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf091777",
      "metadata": {
        "id": "bf091777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, just find a map, close your eyes, throw a dart, and wherever it lands, start walking towards New York from there. If you end up in the ocean, don't worry, just swim with the current until you hit land again and start over. Good luck!\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Option B: assumes MISTRAL_API_KEY is in your environment\n",
        "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=1.0)\n",
        "\n",
        "resp = chat.invoke([\n",
        "    SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says.\"),\n",
        "    HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "])\n",
        "\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c028f9",
      "metadata": {
        "id": "05c028f9"
      },
      "source": [
        "### Function Calling Models\n",
        "\n",
        "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
        "\n",
        "This comes in handy when you're making an API call to an external service or doing extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1020ff45",
      "metadata": {
        "id": "1020ff45",
        "outputId": "ad7c997e-c794-4f57-f0d5-7d24db418a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={'tool_calls': [{'id': 'mRY239U0c', 'function': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston\", \"unit\": \"celsius\"}'}, 'index': 0}]} response_metadata={'token_usage': {'prompt_tokens': 100, 'total_tokens': 120, 'completion_tokens': 20}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'} id='run--a479f082-43b9-4b89-8b9c-a264f74c087e-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Boston', 'unit': 'celsius'}, 'id': 'mRY239U0c', 'type': 'tool_call'}] usage_metadata={'input_tokens': 100, 'output_tokens': 20, 'total_tokens': 120}\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "import json\n",
        "\n",
        "# Define the function as a tool (this replaces functions=[...] in OpenAI)\n",
        "@tool\n",
        "def get_current_weather(location: str, unit: str = \"celsius\") -> dict:\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    return {\n",
        "        \"location\": location,\n",
        "        \"unit\": unit,\n",
        "        \"temperature\": 23,\n",
        "        \"condition\": \"partly cloudy\",\n",
        "    }\n",
        "\n",
        "# Use a valid Mistral model instead of gpt-3.5\n",
        "chat = ChatMistralAI(model=\"mistral-large-latest\", temperature=1)\n",
        "\n",
        "# Bind the tool\n",
        "chat_with_tools = chat.bind_tools([get_current_weather])\n",
        "\n",
        "# Run the model with messages\n",
        "output = chat_with_tools.invoke([\n",
        "    SystemMessage(content=\"You are a helpful AI bot\"),\n",
        "    HumanMessage(content=\"Whatâ€™s the weather like in Boston right now?\")\n",
        "])\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c77ba63",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0f399a1d",
      "metadata": {
        "id": "0f399a1d"
      },
      "source": [
        "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b70f23",
      "metadata": {
        "id": "c2b70f23"
      },
      "source": [
        "### **Text Embedding Model**\n",
        "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
        "\n",
        "*BTW: Semantic means 'relating to meaning in language or logic.'*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1655de82",
      "metadata": {
        "id": "1655de82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a sample: [-0.0302734375, 0.0300445556640625, 0.0487060546875, -0.019012451171875, 0.0269775390625]...\n",
            "Your embedding is length 1024\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "# Create embeddings client (uses MISTRAL_API_KEY from env)\n",
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
        "\n",
        "text = \"Hi! It's time for the beach\"\n",
        "\n",
        "text_embedding = embeddings.embed_query(text)\n",
        "print(f\"Here's a sample: {text_embedding[:5]}...\")\n",
        "print(f\"Your embedding is length {len(text_embedding)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc5a368",
      "metadata": {
        "id": "ddc5a368",
        "outputId": "ab56479e-c260-4544-9c93-8145b0362a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a sample: [-0.00019600906371495047, -0.0031846734422911363, -0.0007734206914647714, -0.019472001962491232, -0.015092319017854244]...\n",
            "Your embedding is length 1536\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c38fe99f",
      "metadata": {
        "id": "c38fe99f"
      },
      "source": [
        "## Prompts - Text generally used as instructions to your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9318ed",
      "metadata": {
        "id": "8b9318ed"
      },
      "source": [
        "### **Prompt**\n",
        "What you'll pass to the underlying model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2d270239",
      "metadata": {
        "id": "2d270239",
        "outputId": "a37f6b8f-3fa0-4cad-bb9b-5a6b94b605d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alright, let's tackle this problem step by step. The statement given is:\n",
            "\n",
            "**\"Today is Monday, tomorrow is Wednesday. What is wrong with that statement?\"**\n",
            "\n",
            "At first glance, this seems straightforward, but there's something amiss here. Let's break it down to understand what's going on.\n",
            "\n",
            "### Understanding the Statement\n",
            "\n",
            "1. **Today is Monday**: This means the current day is Monday.\n",
            "2. **Tomorrow is Wednesday**: This implies that the day after today (Monday) is Wednesday.\n",
            "\n",
            "Now, let's think about the sequence of days in a week to see if this makes sense.\n",
            "\n",
            "### The Sequence of Days in a Week\n",
            "\n",
            "The standard sequence of days in a week is as follows:\n",
            "\n",
            "- Monday\n",
            "- Tuesday\n",
            "- Wednesday\n",
            "- Thursday\n",
            "- Friday\n",
            "- Saturday\n",
            "- Sunday\n",
            "\n",
            "Given that today is Monday, the next day should logically be Tuesday, followed by Wednesday.\n",
            "\n",
            "### Analyzing the Given Statement\n",
            "\n",
            "According to the statement:\n",
            "- Today: Monday\n",
            "- Tomorrow: Wednesday\n",
            "\n",
            "But according to the standard sequence:\n",
            "- Today: Monday\n",
            "- Tomorrow: Tuesday\n",
            "- Day after tomorrow: Wednesday\n",
            "\n",
            "So, the statement skips Tuesday, making tomorrow Wednesday instead of Tuesday. That seems to be the inconsistency.\n",
            "\n",
            "### Possible Explanations\n",
            "\n",
            "1. **Typo or Misstatement**: It could be a simple mistake where someone meant to say \"tomorrow is Tuesday\" but mistakenly said \"Wednesday.\"\n",
            "\n",
            "2. **Alternative Calendar or Timekeeping**: Maybe the person is using a different calendar system where the days are named differently or the sequence is altered. However, in the Gregorian calendar, which is widely used, the sequence is fixed as mentioned above.\n",
            "\n",
            "3. **Humorous or Rhetorical Statement**: Sometimes, such statements are made to highlight the absurdity or to make a point about how days are named or perceived.\n",
            "\n",
            "4. **Time Travel or Fictional Scenario**: In a fictional context, time might behave differently, but in reality, this doesn't hold.\n",
            "\n",
            "Given that this is a straightforward logical puzzle, the most plausible explanation is that the statement is incorrect because it skips Tuesday, making the sequence of days inconsistent.\n",
            "\n",
            "### Verifying with Examples\n",
            "\n",
            "Let's take another day to see if this pattern holds.\n",
            "\n",
            "Suppose today is Tuesday:\n",
            "- Tomorrow should be Wednesday.\n",
            "If someone says \"today is Tuesday, tomorrow is Thursday,\" that would similarly skip Wednesday, which is incorrect.\n",
            "\n",
            "Similarly, if today is Wednesday:\n",
            "- Tomorrow should be Thursday.\n",
            "Saying \"tomorrow is Friday\" would skip Thursday, which is incorrect.\n",
            "\n",
            "This pattern shows that the statement is incorrect because it skips the day that should come immediately after today.\n",
            "\n",
            "### Considering the Possibility of a Different Week Start\n",
            "\n",
            "Some cultures or calendars might start the week on a different day, like Sunday. But even then, the sequence would be:\n",
            "\n",
            "- Sunday\n",
            "- Monday\n",
            "- Tuesday\n",
            "- Wednesday\n",
            "- Thursday\n",
            "- Friday\n",
            "- Saturday\n",
            "\n",
            "If today is Sunday:\n",
            "- Tomorrow is Monday, not Wednesday.\n",
            "\n",
            "If today is Monday:\n",
            "- Tomorrow is Tuesday, not Wednesday.\n",
            "\n",
            "So, regardless of the starting day, the sequence is consistent, and skipping a day doesn't make sense in any standard calendar system.\n",
            "\n",
            "### Exploring Other Interpretations\n",
            "\n",
            "Could there be another way to interpret \"today is Monday, tomorrow is Wednesday\"?\n",
            "\n",
            "One might think about the phrase \"tomorrow is Wednesday\" as a prediction or a statement about the future, but in reality, the sequence is fixed, and predictions can't change the order of days.\n",
            "\n",
            "Alternatively, someone might be joking or making a playful statement, but in the context of a logical puzzle, we're looking for the factual inconsistency.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "After carefully considering all these points, the most straightforward and logical answer is that the statement is incorrect because it skips Tuesday, making the sequence from Monday to Wednesday instead of Monday to Tuesday to Wednesday.\n",
            "\n",
            "### Final Answer\n",
            "\n",
            "The statement \"Today is Monday, tomorrow is Wednesday\" is incorrect because it incorrectly skips Tuesday. If today is Monday, the next day (tomorrow) should be Tuesday, not Wednesday. The sequence of days in a week is Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday. Skipping Tuesday makes the statement logically inconsistent.\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Create the model (uses MISTRAL_API_KEY from env by default)\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
        "\n",
        "# Triple-quoted prompt\n",
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\"\n",
        "\n",
        "resp = llm.invoke(prompt)\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74988254",
      "metadata": {
        "id": "74988254"
      },
      "source": [
        "### **Prompt Template**\n",
        "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
        "\n",
        "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts\n",
        "\n",
        "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "abcc212d",
      "metadata": {
        "id": "abcc212d",
        "outputId": "400bfc36-538c-4122-ef53-cdd396b9fe85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/__init__.py:24: UserWarning: Importing PromptTemplate from langchain root module is no longer supported.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Rome. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: Explore ancient ruins like the Colosseum, toss a coin in the Trevi Fountain, and savor authentic pasta carbonara.\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Use a valid Mistral model\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
        "\n",
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Rome')\n",
        "\n",
        "print(f\"Final Prompt: {final_prompt}\")\n",
        "print(\"-----------\")\n",
        "\n",
        "resp = llm.invoke(final_prompt)\n",
        "print(f\"LLM Output: {resp.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed40bac2",
      "metadata": {
        "id": "ed40bac2"
      },
      "source": [
        "### **Example Selectors**\n",
        "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
        "\n",
        "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
        "\n",
        "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "aaf36cd9",
      "metadata": {
        "id": "aaf36cd9",
        "outputId": "27fb9515-691f-4b9b-dfde-ac1203d78d7f"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Use valid Mistral model\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of locations that nouns are found\n",
        "examples = [\n",
        "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
        "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
        "    {\"input\": \"driver\", \"output\": \"car\"},\n",
        "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
        "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "12b4798b",
      "metadata": {
        "id": "12b4798b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "\n",
        "    # Embedding class (uses your MISTRAL_API_KEY from env by default)\n",
        "    MistralAIEmbeddings(model=\"mistral-embed\"),\n",
        "\n",
        "    # VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    Chroma,\n",
        "\n",
        "    # Number of examples to produce.\n",
        "    k=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2cf30107",
      "metadata": {
        "id": "2cf30107"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # The object that will help select examples dynamically\n",
        "    example_selector=example_selector,\n",
        "\n",
        "    # The mini-prompt template for each example\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    # Customizations that will be added to the top and bottom of your prompt\n",
        "    prefix=\"Give the location an item is usually found in\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "\n",
        "    # What inputs your prompt will receive\n",
        "    input_variables=[\"noun\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "369442bb",
      "metadata": {
        "id": "369442bb",
        "outputId": "8a8e5619-0437-4d3b-af38-223f8d5c9f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the location an item is usually found in\n",
            "\n",
            "Example Input: bird\n",
            "Example Output: nest\n",
            "\n",
            "Example Input: tree\n",
            "Example Output: ground\n",
            "\n",
            "Input: plant\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "# Select a noun!\n",
        "my_noun = \"plant\"\n",
        "# my_noun = \"student\"\n",
        "\n",
        "# The FewShotPromptTemplate will fill in examples and format the final prompt\n",
        "print(similar_prompt.format(noun=my_noun))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9bb910f2",
      "metadata": {
        "id": "9bb910f2",
        "outputId": "068b1120-263c-4e9b-80b7-20259316a04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            " Give the location an item is usually found in\n",
            "\n",
            "Example Input: bird\n",
            "Example Output: nest\n",
            "\n",
            "Example Input: tree\n",
            "Example Output: ground\n",
            "\n",
            "Input: plant\n",
            "Output:\n",
            "-----------\n",
            "LLM Output: pot\n",
            "\n",
            "(Plants are commonly found in pots, especially when grown indoors or in gardens.)\n"
          ]
        }
      ],
      "source": [
        "# Generate the few-shot prompt\n",
        "prompt_text = similar_prompt.format(noun=my_noun)\n",
        "\n",
        "# Send to the Mistral model\n",
        "resp = llm.invoke(prompt_text)\n",
        "\n",
        "print(\"Prompt:\\n\", prompt_text)\n",
        "print(\"-----------\")\n",
        "print(\"LLM Output:\", resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8474c91d",
      "metadata": {
        "id": "8474c91d"
      },
      "source": [
        "### **Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
        "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their [documentation](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
        "\n",
        "Two big concepts:\n",
        "\n",
        "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
        "\n",
        "**2. Parser** - A method which will extract your model's text output into a desired structure (usually json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "58353756",
      "metadata": {
        "id": "58353756"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_mistralai import ChatMistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ee36f881",
      "metadata": {
        "id": "ee36f881"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Correct Mistral model, no stray args\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fa59be3f",
      "metadata": {
        "id": "fa59be3f"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "# How you would like your response structured. This is basically a fancy prompt template\n",
        "response_schemas = [\n",
        "    ResponseSchema(\n",
        "        name=\"bad_string\", \n",
        "        description=\"This a poorly formatted user input string\"\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"good_string\", \n",
        "        description=\"This is your response, a reformatted response\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# How you would like to parse your output\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d1079f0a",
      "metadata": {
        "id": "d1079f0a",
        "outputId": "39fe42fc-eef5-489f-d18b-56560270ebf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // This a poorly formatted user input string\n",
            "\t\"good_string\": string  // This is your response, a reformatted response\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# See the prompt template you created for formatting\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9aaae5be",
      "metadata": {
        "id": "9aaae5be",
        "outputId": "4ce0d3c9-e0e5-4f77-b357-722f9f8e136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You will be given a poorly formatted string from a user.\n",
            "Reformat it and make sure all the words are spelled correctly.\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // This a poorly formatted user input string\n",
            "\t\"good_string\": string  // This is your response, a reformatted response\n",
            "}\n",
            "```\n",
            "\n",
            "% USER INPUT:\n",
            "welcom to califonya!\n",
            "\n",
            "YOUR RESPONSE:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You will be given a poorly formatted string from a user.\n",
        "Reformat it and make sure all the words are spelled correctly.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template\n",
        ")\n",
        "\n",
        "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
        "\n",
        "print(promptValue)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b116bb23",
      "metadata": {
        "id": "b116bb23",
        "outputId": "45c4fa08-71c5-41f3-84c0-446f39d3ce4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw output:\n",
            " ```json\n",
            "{\n",
            "\t\"bad_string\": \"welcom to califonya!\",\n",
            "\t\"good_string\": \"Welcome to California!\"\n",
            "}\n",
            "```\n",
            "\n",
            "Parsed output:\n",
            " {'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}\n"
          ]
        }
      ],
      "source": [
        "# Send the formatted prompt to Mistral\n",
        "resp = llm.invoke(promptValue)\n",
        "\n",
        "# See the raw model output\n",
        "print(\"Raw output:\\n\", resp.content)\n",
        "\n",
        "# Parse into structured JSON\n",
        "parsed = output_parser.parse(resp.content)\n",
        "print(\"\\nParsed output:\\n\", parsed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985aa814",
      "metadata": {
        "id": "985aa814",
        "outputId": "cdc3e644-ba2d-4db3-9e63-7ea3e11946b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "07045ae3",
      "metadata": {
        "id": "07045ae3"
      },
      "source": [
        "### **Output Parsers Method 2: OpenAI Fuctions**\n",
        "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
        "\n",
        "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
        "\n",
        "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
        "\n",
        "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use `gpt4-0613`\n",
        "\n",
        "**Example 1: Simple**\n",
        "\n",
        "Let's get started by defining a simple model for us to extract from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3593699b",
      "metadata": {
        "id": "3593699b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Alice' age=30 fav_food='Pizza'\n",
            "{'name': 'Alice', 'age': 30, 'fav_food': 'Pizza'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Identifying information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The person's name\")\n",
        "    age: int = Field(..., description=\"The person's age\")\n",
        "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")\n",
        "\n",
        "# Example usage\n",
        "example = Person(name=\"Alice\", age=30, fav_food=\"Pizza\")\n",
        "print(example)\n",
        "print(example.dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17033d15",
      "metadata": {
        "id": "17033d15"
      },
      "source": [
        "Then let's create a chain (more on this later) that will do the extracting for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "60b7be09",
      "metadata": {
        "id": "60b7be09",
        "outputId": "da8d739c-541b-44ec-8e11-823d47347433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Sally' age=13 fav_food=None\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Use a valid Mistral model\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
        "\n",
        "# Parser that will output a Person object\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "Extract the person's information from the text.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Respond ONLY with a valid JSON object, no extra text.\n",
        "\n",
        "Text: {text}\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# Run the \"chain\"\n",
        "input_text = \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
        "prompt_value = prompt.format(text=input_text)\n",
        "\n",
        "resp = llm.invoke(prompt_value)\n",
        "\n",
        "# Parse into your Person model\n",
        "person = parser.parse(resp.content)\n",
        "print(person)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37370210",
      "metadata": {
        "id": "37370210"
      },
      "source": [
        "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "df4ad5e6",
      "metadata": {
        "id": "df4ad5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "people=[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)]\n",
            "{'people': [{'name': 'Sally', 'age': 13, 'fav_food': None}, {'name': 'Joey', 'age': 12, 'fav_food': 'spinach'}, {'name': 'Caroline', 'age': 23, 'fav_food': None}]}\n"
          ]
        }
      ],
      "source": [
        "from typing import Sequence\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Person stays the same as before\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Identifying information about a person.\"\"\"\n",
        "    name: str = Field(..., description=\"The person's name\")\n",
        "    age: int = Field(..., description=\"The person's age\")\n",
        "    fav_food: str | None = Field(None, description=\"The person's favorite food\")\n",
        "\n",
        "# People wraps a list/sequence of Person\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "    people: Sequence[Person] = Field(..., description=\"The people in the text\")\n",
        "\n",
        "# Parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt that instructs the model to output JSON matching People\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "Extract all people and their information from the text.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Respond ONLY with valid JSON.\n",
        "\n",
        "Text: {text}\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "# Mistral model\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
        "\n",
        "# Input text\n",
        "input_text = \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
        "\n",
        "# Format prompt\n",
        "prompt_value = prompt.format(text=input_text)\n",
        "\n",
        "# Call model\n",
        "resp = llm.invoke(prompt_value)\n",
        "\n",
        "# Parse into People object\n",
        "people = parser.parse(resp.content)\n",
        "print(people)\n",
        "print(people.dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12db9b8b",
      "metadata": {
        "id": "12db9b8b"
      },
      "source": [
        "Let's do some more parsing with it\n",
        "\n",
        "**Example 2: Enum**\n",
        "\n",
        "Now let's parse when a product from a list is mentioned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6616a735",
      "metadata": {
        "id": "6616a735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>]\n",
            "{'products': [<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>]}\n"
          ]
        }
      ],
      "source": [
        "import enum\n",
        "from typing import Sequence\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Enum for product types\n",
        "class Product(str, enum.Enum):\n",
        "    CRM = \"CRM\"\n",
        "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
        "    HARDWARE = \"HARDWARE\"\n",
        "\n",
        "# Pydantic model for list of products\n",
        "class Products(BaseModel):\n",
        "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
        "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")\n",
        "\n",
        "# Parser for Products schema\n",
        "parser = PydanticOutputParser(pydantic_object=Products)\n",
        "\n",
        "# Prompt template with format instructions\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "Extract the products mentioned in the following text.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Respond ONLY with valid JSON.\n",
        "\n",
        "Text: {text}\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "# Use Mistral model\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
        "\n",
        "# Input\n",
        "input_text = \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
        "\n",
        "# Format the prompt\n",
        "prompt_value = prompt.format(text=input_text)\n",
        "\n",
        "# Call Mistral\n",
        "resp = llm.invoke(prompt_value)\n",
        "\n",
        "# Parse output into Products object\n",
        "products = parser.parse(resp.content)\n",
        "print(products)\n",
        "print(products.dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b43cec2",
      "metadata": {
        "id": "7b43cec2"
      },
      "source": [
        "## Indexes - Structuring documents to LLMs can work with them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f904e9",
      "metadata": {
        "id": "d3f904e9"
      },
      "source": [
        "### **Document Loaders**\n",
        "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
        "\n",
        "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4719d4",
      "metadata": {
        "id": "7d4719d4"
      },
      "source": [
        "**HackerNews**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c564583f",
      "metadata": {
        "id": "c564583f"
      },
      "source": [
        "**Books from Gutenberg Project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "72964fb8",
      "metadata": {
        "id": "72964fb8"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import GutenbergLoader\n",
        "\n",
        "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "47140a26",
      "metadata": {
        "id": "47140a26",
        "outputId": "a543cd64-60ff-4ba2-c725-73953e8cf48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "o.â€”_Seneca_.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      At Paris, just after dark one gusty evening in the autumn of 18-,\n",
            "\n",
            "\n",
            "      I was enjoying the twofold l\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content[1855:1984])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f1386b0",
      "metadata": {
        "id": "7f1386b0"
      },
      "source": [
        "**URLs and webpages**\n",
        "\n",
        "Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "46a54e7d",
      "metadata": {
        "id": "46a54e7d",
        "outputId": "9687fd6c-a256-4a99-9440-f27d8cd0668d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'New:  \\n\\n Good Writing  |\\n Founder Mode \\n \\n \\n \\n \\n \\n Want to start a startup?  Get funded by  Y Combinator .\\n \\n \\n \\n\\n \\n\\n \\n \\n \\nÂ© mmxxv pg'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "urls = [\n",
        "    \"http://www.paulgraham.com/\",\n",
        "]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9601db",
      "metadata": {
        "id": "0e9601db"
      },
      "source": [
        "### **Text Splitters**\n",
        "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
        "\n",
        "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "95713e57",
      "metadata": {
        "id": "95713e57"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "a54455f5",
      "metadata": {
        "id": "a54455f5",
        "outputId": "7189d0f9-5691-4305-8f6c-b8810a0c325f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/PaulGrahamEssays/worked.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is a long document we can split up.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/PaulGrahamEssays/worked.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     pg_work \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m([pg_work])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/PaulGrahamEssays/worked.txt'"
          ]
        }
      ],
      "source": [
        "# This is a long document we can split up.\n",
        "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
        "    pg_work = f.read()\n",
        "\n",
        "print (f\"You have {len([pg_work])} document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "d19acb18",
      "metadata": {
        "id": "d19acb18"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pg_work' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Set a really small chunk size, just to show.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m      4\u001b[0m     chunk_overlap  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([\u001b[43mpg_work\u001b[49m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pg_work' is not defined"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap  = 20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([pg_work])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3090f05",
      "metadata": {
        "id": "e3090f05",
        "outputId": "f7525455-8f57-4c8d-ecb6-7461f1b006a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 610 documents\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a0f45a",
      "metadata": {
        "id": "87a0f45a",
        "outputId": "606a6093-7e44-4957-e8af-d91b11adcfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview:\n",
            "February 2021Before college the two main things I worked on, outside of school,\n",
            "were writing and programming. I didn't write essays. I wrote what \n",
            "\n",
            "beginning writers were supposed to write then, and probably still\n",
            "are: short stories. My stories were awful. They had hardly any plot,\n"
          ]
        }
      ],
      "source": [
        "print (\"Preview:\")\n",
        "print (texts[0].page_content, \"\\n\")\n",
        "print (texts[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9e670d",
      "metadata": {
        "id": "ad9e670d"
      },
      "source": [
        "There are a ton of different ways to do text splitting and it really depends on your retrieval strategy and application design. Check out more splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f85defb",
      "metadata": {
        "id": "1f85defb"
      },
      "source": [
        "### **Retrievers**\n",
        "Easy way to combine documents with language models.\n",
        "\n",
        "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cccbd82",
      "metadata": {
        "id": "8cccbd82"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dab1c20",
      "metadata": {
        "id": "1dab1c20"
      },
      "outputs": [],
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = MistralAIEmbeddings(mistral_)\n",
        "\n",
        "# Embedd your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62372be",
      "metadata": {
        "id": "e62372be"
      },
      "outputs": [],
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0534bbd",
      "metadata": {
        "id": "e0534bbd",
        "outputId": "7f30afd4-139a-4e52-b3c7-8b8444c26b87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f8389169070>)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3846a3b5",
      "metadata": {
        "id": "3846a3b5"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db383cc8",
      "metadata": {
        "id": "db383cc8",
        "outputId": "85ab2e2e-e813-469a-b31b-308f4f8a953d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "standards; what was the point? No one else wanted one either, so\n",
            "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
            "last.In this di\n",
            "\n",
            "much of it in grad school.Computer Science is an uneasy alliance between two halves, theory\n",
            "and systems. The theory people prove things, and the systems people\n",
            "build things. I wanted to build things. \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24193139",
      "metadata": {
        "id": "24193139"
      },
      "source": [
        "### **VectorStores**\n",
        "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
        "\n",
        "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
        "\n",
        "Example\n",
        "\n",
        "| Embedding      | Metadata |\n",
        "| ----------- | ----------- |\n",
        "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
        "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5533ad",
      "metadata": {
        "id": "3c5533ad"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "# Load a text file into documents\n",
        "loader = TextLoader(\"data/PaulGrahamEssays/worked.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Prepare a text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into smaller chunks\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create Mistral embeddings (uses MISTRAL_API_KEY from your environment by default)\n",
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
        "\n",
        "# Store in FAISS\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "print(f\"Loaded {len(texts)} chunks into FAISS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661fdf19",
      "metadata": {
        "id": "661fdf19",
        "outputId": "aec432b6-b02d-437a-a864-7c94571c4f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 78 documents\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99ac0ea",
      "metadata": {
        "id": "e99ac0ea"
      },
      "outputs": [],
      "source": [
        "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e7758c",
      "metadata": {
        "id": "89e7758c",
        "outputId": "7f7c19e5-c2fa-427a-8535-457be46f0f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 78 embeddings\n",
            "Here's a sample of one: [-0.001058628615053026, -0.01118234211553424, -0.012874804746266883]...\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(embedding_list)} embeddings\")\n",
        "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac358c5",
      "metadata": {
        "id": "8ac358c5"
      },
      "source": [
        "Your vectorstore store your embeddings (â˜ï¸) and make them easily searchable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b9b79b",
      "metadata": {
        "id": "f9b9b79b"
      },
      "source": [
        "## Memory\n",
        "Helping LLMs remember information.\n",
        "\n",
        "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
        "\n",
        "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
        "\n",
        "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43b49da",
      "metadata": {
        "id": "f43b49da"
      },
      "source": [
        "### Chat Message History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "893a18c1",
      "metadata": {
        "id": "893a18c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello! The capital of France is **Paris**. ðŸ‡«ðŸ‡·\\n\\nWould you like to know more about Paris or France? ðŸ˜Š' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 12, 'total_tokens': 45, 'completion_tokens': 33}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'} id='run--3e462706-d17f-4fd5-b293-f5dde877e6a0-0' usage_metadata={'input_tokens': 12, 'output_tokens': 33, 'total_tokens': 45}\n",
            "[AIMessage(content='hi!', additional_kwargs={}, example=False), HumanMessage(content='what is the capital of france?', additional_kwargs={}, example=False), AIMessage(content='Hello! The capital of France is **Paris**. ðŸ‡«ðŸ‡·\\n\\nWould you like to know more about Paris or France? ðŸ˜Š', additional_kwargs={}, example=False)]\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage  # <- use these\n",
        "\n",
        "# Create the chat model (uses MISTRAL_API_KEY from env)\n",
        "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
        "\n",
        "# Initialize message history\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "# Add messages\n",
        "history.add_ai_message(\"hi!\")\n",
        "history.add_user_message(\"what is the capital of france?\")\n",
        "\n",
        "# Convert history messages into the right type\n",
        "messages = [\n",
        "    AIMessage(content=m.content) if isinstance(m, AIMessage) else HumanMessage(content=m.content)\n",
        "    for m in history.messages\n",
        "]\n",
        "\n",
        "# Get AI response\n",
        "ai_response = chat.invoke(messages)\n",
        "print(ai_response)\n",
        "\n",
        "# Add AI response back to history\n",
        "history.add_ai_message(ai_response.content)\n",
        "\n",
        "print(history.messages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f6193c",
      "metadata": {
        "id": "84f6193c"
      },
      "source": [
        "## Agents ðŸ¤–ðŸ¤–\n",
        "\n",
        "Official LangChain Documentation describes agents perfectly (emphasis mine):\n",
        "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a â€œagentâ€ which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
        "\n",
        "\n",
        "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
        "\n",
        "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce05d51",
      "metadata": {
        "id": "3ce05d51"
      },
      "source": [
        "### Agents\n",
        "\n",
        "The language model that drives decision making.\n",
        "\n",
        "More specifically, an agent takes in an input and returns a response corresponding to an action to take along with an action input. You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f696b65c",
      "metadata": {
        "id": "f696b65c"
      },
      "source": [
        "### Tools\n",
        "\n",
        "A 'capability' of an agent. This is an abstraction on top of a function that makes it easy for LLMs (and agents) to interact with it. Ex: Google search.\n",
        "\n",
        "This area shares commonalities with [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a11f8231",
      "metadata": {
        "id": "a11f8231"
      },
      "source": [
        "### Toolkit\n",
        "\n",
        "Groups of tools that your agent can select from\n",
        "\n",
        "Let's bring them all together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d5d82d",
      "metadata": {
        "id": "67d5d82d"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for SerpAPIWrapper\n__root__\n  Could not import serpapi python package. Please install it with `pip install google-search-results`. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[67], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m serpapi_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSERP_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYourSerpApiKey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the search tool\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserpapi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserpapi_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserpapi_api_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize the agent\u001b[39;00m\n\u001b[1;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     17\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtoolkit,\n\u001b[1;32m     18\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     return_intermediate_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/agents/load_tools.py:472\u001b[0m, in \u001b[0;36mload_tools\u001b[0;34m(tool_names, llm, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     _get_tool_func, extra_keys \u001b[38;5;241m=\u001b[39m _EXTRA_OPTIONAL_TOOLS[name]\n\u001b[1;32m    471\u001b[0m     sub_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extra_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[0;32m--> 472\u001b[0m     tool \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tool_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msub_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(tool)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/agents/load_tools.py:221\u001b[0m, in \u001b[0;36m_get_serpapi\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_serpapi\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tool(\n\u001b[1;32m    219\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA search engine. Useful for when you need to answer questions about current events. Input should be a search query.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 221\u001b[0m         func\u001b[38;5;241m=\u001b[39m\u001b[43mSerpAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrun,\n\u001b[1;32m    222\u001b[0m         coroutine\u001b[38;5;241m=\u001b[39mSerpAPIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39marun,\n\u001b[1;32m    223\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for SerpAPIWrapper\n__root__\n  Could not import serpapi python package. Please install it with `pip install google-search-results`. (type=value_error)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Create the Mistral model (uses MISTRAL_API_KEY from env)\n",
        "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
        "\n",
        "# Get SERP API key (from env or fallback)\n",
        "serpapi_api_key = os.getenv(\"SERP_API_KEY\", \"YourSerpApiKey\") # need the key\n",
        "\n",
        "\n",
        "# Load the search tool\n",
        "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=toolkit,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True,\n",
        "    return_intermediate_steps=True,\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "response = agent({\"input\": \"what was the first album of the band that Natalie Bergman is a part of?\"})\n",
        "\n",
        "print(json.dumps(response, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9c30d2",
      "metadata": {
        "id": "3f9c30d2"
      },
      "source": [
        "![Wild Belle](https://github.com/gkamradt/langchain-tutorials/blob/main/data/WildBelle1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f4b368",
      "metadata": {
        "id": "14f4b368"
      },
      "source": [
        "ðŸŽµEnjoyðŸŽµ\n",
        "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch311clean",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

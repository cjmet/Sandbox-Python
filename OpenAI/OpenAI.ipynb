{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9967e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "## Global Options\n",
    "default_model = \"gpt-5-nano\" \n",
    "minimum_tokens = 2500   \n",
    "rate_limit_seconds = 3  \n",
    "default_debug = False   \n",
    "\n",
    "# def generate_response(prompt, model=None, temperature=None, max_tokens=None, top_p=None, \n",
    "#                       seed=None, effort=\"minimal\", use_completions=False, service_tier=\"flex\", \n",
    "#                       previous_response_id=None, prompt_instructions=None, debug=None):\n",
    "#    return {\"text\": response.output_text, \"id\": response.id, \"model\": model, \"service_tier\": service_tier, \"usage\": response.usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a15a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "rate_limit_timeout = 0  # When the rate limit will expire\n",
    "\n",
    "def get_global_settings_string(): \n",
    "     if not os.environ.get(\"OPENAI_API\"):\n",
    "          raise ValueError(\"OPENAI_API environment variable is not set. Please set it to your API key.\")\n",
    "     return f\"{default_model} {minimum_tokens}tk {rate_limit_seconds}s { 'debug' if default_debug else '' }\"\n",
    "\n",
    "def divider(): \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1013cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def display_cost_human(cost): # GenAI\n",
    "    \"\"\"\n",
    "    Display cost in human-readable format:\n",
    "    - If cost >= $1, show as $0.00\n",
    "    - If cost >= $0.01 and < $1, show as 0.00¢\n",
    "    - If cost < $0.01, show as fractional cents: 1/100¢, 1/1,000¢, etc.\n",
    "    \"\"\"\n",
    "    if cost >= 0.994:\n",
    "        return f\"${cost:,.2f}\"\n",
    "    elif cost > 0.001:\n",
    "        cost = math.ceil(cost * 100) / 100  # Round up to nearest cent\n",
    "        return f\"{cost * 100:,.0f}¢\"\n",
    "    else:\n",
    "        # Find denominator for fractional cent\n",
    "        denominator = 1000\n",
    "        while cost < 1 / denominator:\n",
    "            denominator *= 10\n",
    "        denominator //= 100  # Adjust to the last valid denominator\n",
    "        return f\"1/{denominator:,}¢\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_of_openai_api_call(model, input_tokens, output_tokens, flex=False):\n",
    "    # Token cost rates per million\n",
    "    token_cost = {\n",
    "        \"gpt-5\": (1.25, 10),\n",
    "        \"gpt-5-chat-latest\": (1.25, 10),\n",
    "        \"gpt-5-mini\": (0.25, 2),\n",
    "        \"gpt-5-nano\": (0.05, 0.4),\n",
    "        \"gpt-4.1\": (2.00, 8),\n",
    "        \"gpt-4.1-mini\": (0.4, 1.6),\n",
    "        \"gpt-4.1-nano\": (0.1, 0.4),\n",
    "        \"o3-deep-research-2025-06-26\": (10, 40),\n",
    "        \"o4-mini-deep-research-2025-06-26\": (2, 8)\n",
    "    }\n",
    "\n",
    "    if model not in token_cost:\n",
    "        raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "    # Calculate cost\n",
    "    input_cost = input_tokens * token_cost[model][0]  # Input token cost\n",
    "    output_cost = output_tokens * token_cost[model][1]  # Output token cost\n",
    "    cost = input_cost + output_cost\n",
    "    if flex: cost *= 0.5  # flex discount\n",
    "    cost /= 1000000 # prices are per million\n",
    "\n",
    "    return display_cost_human(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_of_responses_api(response_dict):\n",
    "    if response_dict == None or response_dict['usage'] == None: return ValueError\n",
    "    model = response_dict['model']\n",
    "    service_tier = response_dict['service_tier']\n",
    "    return (\n",
    "        f\"{model}: {response_dict['usage'].input_tokens} + {response_dict['usage'].output_tokens} = {response_dict['usage'].total_tokens}\\t\"\n",
    "        + cost_of_openai_api_call(model, response_dict['usage'].input_tokens, response_dict['usage'].output_tokens, service_tier)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3357c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_of_completions_api(response_dict):\n",
    "    if response_dict == None or response_dict.usage == None: return ValueError\n",
    "    model = response_dict.model\n",
    "    service_tier = response_dict.service_tier\n",
    "    return (\n",
    "        f\"Usage: {response_dict.usage.prompt_tokens} + {response_dict.usage.completion_tokens} = {response_dict.usage.total_tokens}\\t\"\n",
    "        + cost_of_openai_api_call(model, response_dict.usage.prompt_tokens, response_dict.usage.completion_tokens, service_tier)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae21ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API\"):\n",
    "    raise ValueError(\"OPENAI_API environment variable is not set. Please set it to your API key.\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Function to generate a response \n",
    "\n",
    "## Models to test with gpt-5, gpt-5-chat-latest (chatty), gpt-5-nano (cheap reasoning, no temperature)\n",
    "# seed=seed  # Not supported with client.responses.create\n",
    "\n",
    "def generate_response(prompt, model=None, temperature=None, max_tokens=None, top_p=None, \n",
    "                      seed=None, effort=\"minimal\", use_completions=False, service_tier=\"flex\", \n",
    "                      previous_response_id=None, prompt_instructions=None, debug=None):\n",
    "    # Configurable Defaults\n",
    "    if model == None: model = default_model  # Use default model if not specified\n",
    "    if model not in [\"gpt-5\", \"gpt-5-chat-latest\", \"gpt-5-mini\", \"gpt-5-nano\", \"gpt-4.1\", \"gpt-4.1-mini\" ,\"gpt-4.1-nano\"]:\n",
    "        raise ValueError(f\"Model {model} is not supported.\") \n",
    "    if max_tokens == None: max_tokens = minimum_tokens  # Use minimum tokens if not specified  \n",
    "    if max_tokens <= 0 : max_tokens = None # Use None for unlimited tokens\n",
    "    if debug == None: debug = default_debug  # Use default debug setting if not specified\n",
    "\n",
    "    # NOT supported on gpt-5, convert gpt-4.1\n",
    "    if temperature != None or top_p != None or seed != None:\n",
    "        model = model.replace(\"5\", \"4.1\")  # Use gpt-4.1 for completions\n",
    "        model = model.replace(\"-chat-latest\", \"\") \n",
    "    # NOT Supported on Responses API\n",
    "    if seed != None:\n",
    "        use_completions = True\n",
    "    # NOT Supported on Completions API\n",
    "    if use_completions: \n",
    "        effort = None \n",
    "    # NOT Supported on gpt-4.1\n",
    "    if model.startswith(\"gpt-4.1\"):\n",
    "        effort = None\n",
    "        service_tier = \"default\"\n",
    "    # NOT Supported on nano - \n",
    "    if model.endswith(\"nano\"):\n",
    "        service_tier = \"default\" # This is supposed to be supported on 5-nano, but does not appear to be functional today.\n",
    "\n",
    "    reasoning={\"effort\": effort} if effort != None else None\n",
    "\n",
    "\n",
    "    # DEBUG Message\n",
    "    if debug:\n",
    "        message = f\"{model} \" + f\"{'C' if use_completions else 'R'} \" + f\"{max_tokens}tk \"\n",
    "        if temperature != None: message += f\"{temperature}° \"\n",
    "        if top_p != None: message += f\"{top_p}p \"\n",
    "        if seed != None: message += f\"{seed}r \"\n",
    "        if effort != None: message += f\"{effort} \"\n",
    "        if service_tier: message += f\"{service_tier} \"\n",
    "        print(message)\n",
    "    # /message\n",
    "\n",
    " \n",
    "    # Rate limit\n",
    "    global rate_limit_seconds\n",
    "    global rate_limit_timeout\n",
    "    now = time.time()\n",
    "    dif = rate_limit_timeout - now   \n",
    "\n",
    "    # Spinner \n",
    "    if (dif > 0):\n",
    "        # Spinner Countdown   \n",
    "        while dif > 0:\n",
    "            print(f\"\\rRate limit {dif:5.3f}s\", end='', flush=True)\n",
    "            time.sleep(0.25)  # Update spinner every 0.25 seconds\n",
    "            dif = rate_limit_timeout - time.time()\n",
    "        print(\"\\r                         \\r\", end='', flush=True)\n",
    "        # /Spinner \n",
    "\n",
    "    # Set the Rate Limit    \n",
    "    rate_limit_timeout = time.time() + rate_limit_seconds\n",
    "    \n",
    "\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API\"],\n",
    "                    )\n",
    "\n",
    "    try:\n",
    "        if use_completions:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_completion_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                seed=seed,\n",
    "                service_tier=service_tier,\n",
    "                safety_identifier=\"test_user\",\n",
    "                prompt_cache_key=\"test_user\",\n",
    "                # reasoning=reasoning,\n",
    "                previous_response_id=previous_response_id,\n",
    "                instructions=prompt_instructions,\n",
    "            )\n",
    "        else:\n",
    "            response = client.responses.create(\n",
    "            model=model,\n",
    "            input=prompt,\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_tokens,\n",
    "            top_p=top_p,  \n",
    "            # seed=seed,\n",
    "            service_tier=service_tier,  \n",
    "            safety_identifier=\"test_user\",\n",
    "            prompt_cache_key=\"test_user\",\n",
    "            reasoning=reasoning,\n",
    "            previous_response_id=previous_response_id,\n",
    "            instructions=prompt_instructions,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    if use_completions:\n",
    "        if debug: print(f\"Usage: {response.usage.prompt_tokens} + {response.usage.completion_tokens} = {response.usage.total_tokens}\\t\", \n",
    "                        cost_of_openai_api_call(model, response.usage.prompt_tokens, response.usage.completion_tokens, service_tier))\n",
    "        return {\"text\": response.choices[0].message.content, \"id\": response.id, \"model\": model, \"service_tier\": service_tier, \"usage\": response.usage}\n",
    "    else:\n",
    "        if debug: print(f\"Usage: {response.usage.input_tokens} + {response.usage.output_tokens} = {response.usage.total_tokens}\\t\", \n",
    "                        cost_of_openai_api_call(model, response.usage.input_tokens, response.usage.output_tokens, service_tier))\n",
    "        return {\"text\": response.output_text, \"id\": response.id, \"model\": model, \"service_tier\": service_tier, \"usage\": response.usage}\n",
    "\n",
    "# /------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

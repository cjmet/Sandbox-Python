{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9967e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Options\n",
    "rate_limit_seconds = 3  # API Rate limit in seconds\n",
    "rate_limit_timeout = 0  # When the rate limit will expire\n",
    "\n",
    "def divider(): \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae21ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is set.\n",
      "\n",
      "Generating response with model: gpt-5-nano, temperature: None, max_tokens: 1000, top_p: None, \n",
      "Usage: 17 + 382 = 399\n",
      "\n",
      "Prompt: Testing 1, 2, 3, ...\n",
      "Response: Loud and clear. What would you like to test or do today? I can help with Q&A, writing, summarizing, coding, translating, or analyzing data. If you want to test image input, you can share an image and I’ll describe or analyze it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API\"):\n",
    "    raise ValueError(\"OPENAI_API environment variable is not set. Please set it to your API key.\")\n",
    "else : \n",
    "    print(\"API Key is set.\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Function to generate a response \n",
    "\n",
    "## Models to test with gpt-5, gpt-5-chat-latest (chatty), gpt-5-nano (cheap reasoning, no temperature)\n",
    "# seed=seed  # Not supported with client.responses.create\n",
    "\n",
    "def generate_response(prompt, model=\"gpt-5-nano\", temperature=0.7, max_tokens=200, top_p=1.0, debug=False):\n",
    "\n",
    "    # Adjust parameters for models\n",
    "    if model == \"gpt-5-nano\":\n",
    "        if max_tokens < 1000 : \n",
    "            max_tokens = 1000    # gpt-5-nano requires at least 1000 tokens\n",
    "        if temperature != None: \n",
    "            temperature = None  # gpt-5-nano does not support temperature\n",
    "            if debug: print(\"Temperature Not Supported\")\n",
    "        if top_p != None: \n",
    "            top_p = None              # gpt-5-nano does not support top_p\n",
    "            if debug: print(\"Top_p Not Supported\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Generating response with model: {model}, temperature: {temperature}, max_tokens: {max_tokens}, top_p: {top_p}, \")\n",
    "\n",
    "    # Rate limit\n",
    "    global rate_limit_seconds\n",
    "    global rate_limit_timeout\n",
    "    now = time.time()\n",
    "    dif = rate_limit_timeout - now   \n",
    "\n",
    "    # Spinner \n",
    "    if (dif > 0):\n",
    "        # Spinner Countdown   \n",
    "        while dif > 0:\n",
    "            print(f\"\\rRate limit {dif:5.3f}s\", end='', flush=True)\n",
    "            time.sleep(0.25)  # Update spinner every 0.25 seconds\n",
    "            dif = rate_limit_timeout - time.time()\n",
    "        print(\"\\r                         \\r\", end='', flush=True)\n",
    "        # /Spinner \n",
    "\n",
    "    # Set the Rate Limit    \n",
    "    rate_limit_timeout = time.time() + rate_limit_seconds\n",
    "    \n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API\"],\n",
    "                    )\n",
    "\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "        model=model,\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_tokens,\n",
    "        top_p=top_p,  \n",
    "        service_tier=\"flex\",  \n",
    "        safety_identifier=\"test_user\",\n",
    "        prompt_cache_key=\"test_user\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "    if debug:\n",
    "        print(f\"Usage: {response.usage.input_tokens} + {response.usage.output_tokens} = {response.usage.total_tokens}\\n\")\n",
    "    return response.output_text \n",
    "\n",
    "# /------------------------------------------------\n",
    "\n",
    "# Test\n",
    "if (True):  # Set to True to run examples\n",
    "    print()\n",
    "    prompt = \"Testing 1, 2, 3, ...\"\n",
    "    response = generate_response(prompt, debug=True)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "03868577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Prompt\n",
      "Prompt: Explain AI in one sentence.\n",
      "Response: Artificial intelligence is the field of computer science that builds systems capable of learning from data, reasoning about problems, and performing tasks that normally require human intelligence.\n",
      "\n",
      "--------------------------------------------------\n",
      "Zero-shot Prompt         \n",
      "Prompt: Classify this sentiment: 'I love this product!'\n",
      "Response: Positive\n",
      "\n",
      "--------------------------------------------------\n",
      "Few-shot Prompt          \n",
      "Prompt: \n",
      "    Example 1: Text: 'Great movie.' Sentiment: Positive.\n",
      "    Example 2: Text: 'Terrible service.' Sentiment: Negative.\n",
      "    Classify: 'Okay experience.'\n",
      "    \n",
      "Response: Neutral. The phrase expresses a middling sentiment, not clearly positive or negative.\n",
      "\n",
      "--------------------------------------------------\n",
      "CoT Prompt               \n",
      "Prompt: Solve: What is 15% of 200? Think step by step.\n",
      "Response: Step-by-step:\n",
      "\n",
      "- Convert 15% to a decimal: 15% = 0.15\n",
      "- Multiply by 200: 0.15 × 200 = 30\n",
      "\n",
      "Alternative quick check:\n",
      "- 15% = 10% + 5%\n",
      "- 10% of 200 = 20; 5% of 200 = 10; total = 30\n",
      "\n",
      "Answer: 30\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if (True):  # Set to True to run examples\n",
    "    # Example: Basic prompt\n",
    "    basic_prompt = \"Explain AI in one sentence.\"\n",
    "    response = generate_response(basic_prompt)\n",
    "    print(\"Basic Prompt\")\n",
    "    print(f\"Prompt: {basic_prompt}\\nResponse: {response}\\n\")\n",
    "    divider()\n",
    "\n",
    "    # Example: Zero-shot, few-shot, and CoT prompts\n",
    "    zero_shot_prompt = \"Classify this sentiment: 'I love this product!'\"\n",
    "    zero_response = generate_response(zero_shot_prompt, temperature=0.5)\n",
    "    print(\"Zero-shot Prompt\")\n",
    "    print(f\"Prompt: {zero_shot_prompt}\\nResponse: {zero_response}\\n\")\n",
    "    divider()\n",
    "\n",
    "    # Few-shot example\n",
    "    few_shot_prompt = \"\"\"\n",
    "    Example 1: Text: 'Great movie.' Sentiment: Positive.\n",
    "    Example 2: Text: 'Terrible service.' Sentiment: Negative.\n",
    "    Classify: 'Okay experience.'\n",
    "    \"\"\"\n",
    "    few_response = generate_response(few_shot_prompt, top_p=0.9)  # Now supported\n",
    "    print(\"Few-shot Prompt\")\n",
    "    print(f\"Prompt: {few_shot_prompt}\\nResponse: {few_response}\\n\")\n",
    "    divider()\n",
    "\n",
    "    # Chain of Thought (CoT) example\n",
    "    cot_prompt = \"Solve: What is 15% of 200? Think step by step.\"\n",
    "    cot_response = generate_response(cot_prompt, temperature=0.2)\n",
    "    print(\"CoT Prompt\")\n",
    "    print(f\"Prompt: {cot_prompt}\\nResponse: {cot_response}\\n\")\n",
    "    divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6f7819d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of recent edits\n"
     ]
    }
   ],
   "source": [
    "print(\"End of recent edits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
